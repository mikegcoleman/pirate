# LLM API Configuration
# Copy this file to .env and configure for your environment

# Server Configuration
PORT=8080
DEBUG=false

# LLM Backend Configuration
# URL of your LLM server (e.g., Docker Model Runner, OpenAI, etc.)
LLM_BASE_URL=http://localhost:11434

# TTS Configuration
# TTS model to use (see https://tts.readthedocs.io/en/latest/models.html)
TTS_MODEL=tts_models/en/ljspeech/tacotron2-DDC

# Performance Settings
# Timeout for LLM API requests (seconds)
LLM_TIMEOUT=30

# GPU Configuration
# Set to true to use GPU acceleration (requires CUDA)
USE_GPU=true

# Logging Configuration
# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Security Configuration
# Enable CORS for frontend communication
ENABLE_CORS=true

# Example configurations by platform:

# For Docker Model Runner (local LLM):
# LLM_BASE_URL=http://localhost:11434

# For OpenAI:
# LLM_BASE_URL=https://api.openai.com/v1

# For other LLM providers:
# LLM_BASE_URL=https://your-llm-provider.com/api
