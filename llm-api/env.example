# LLM API Configuration
# Copy this file to .env and configure for your environment

# Server Configuration
PORT=8080
DEBUG=false

# LLM Backend Configuration
# URL of your LLM server (e.g., Docker Model Runner, OpenAI, etc.)
LLM_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1

# LLM Model Configuration
# Model to use for chat completions
LLM_MODEL=llama3.2:8b-instruct-q4_K_M

# OpenAI API Configuration (only needed if using OpenAI)
OPENAI_API_KEY=your_openai_api_key_here

# ElevenLabs TTS Configuration
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here
ELEVENLABS_VOICE_ID=your_voice_id_here

# Performance Settings
# Timeout for LLM API requests (seconds)
LLM_TIMEOUT=30

# GPU Configuration (deprecated - no longer used)
# USE_GPU=true

# Logging Configuration
# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Security Configuration
# Enable CORS for frontend communication
ENABLE_CORS=true

# Example configurations:

# For Docker Model Runner (local LLM):
# LLM_BASE_URL=http://localhost:11434
# LLM_MODEL=llama3.2:8b-instruct-q4_K_M

# For OpenAI:
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_MODEL=gpt-4o
# OPENAI_API_KEY=sk-your-actual-api-key-here

# For other LLM providers:
# LLM_BASE_URL=https://your-llm-provider.com/api
