# LLM API Configuration
# Copy this file to .env and configure for your environment

# Server Configuration
PORT=8080
DEBUG=false

# LLM Backend Configuration
# URL of your LLM server (e.g., Docker Model Runner, OpenAI, etc.)
LLM_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1

# LLM Model Configuration
# Model to use for chat completions
LLM_MODEL=llama3.2:8b-instruct-q4_K_M

# TTS Configuration
# TTS Provider: 'kokoro' for local TTS, 'elevenlabs' for cloud TTS
TTS_PROVIDER=kokoro

# Kokoro TTS paths (will auto-download if not present)
KOKORO_MODEL_PATH=./models/kokoro/model.onnx
KOKORO_VOICES_PATH=./models/kokoro/voices-v1.0.bin

# ElevenLabs Configuration (only needed if TTS_PROVIDER=elevenlabs)
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here
ELEVENLABS_VOICE_ID=your_voice_id_here

# Fallback message path
FALLBACK_MESSAGE_PATH=./assets/fallback_message_b64.txt

# Performance Settings
# Timeout for LLM API requests (seconds)
LLM_TIMEOUT=30

# GPU Configuration
# Set to true to use GPU acceleration (requires CUDA)
USE_GPU=true

# Logging Configuration
# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Security Configuration
# Enable CORS for frontend communication
ENABLE_CORS=true

# Example configurations:

# For Docker Model Runner (local LLM):
# LLM_BASE_URL=http://localhost:11434

# For OpenAI:
# LLM_BASE_URL=https://api.openai.com/v1

# For other LLM providers:
# LLM_BASE_URL=https://your-llm-provider.com/api
