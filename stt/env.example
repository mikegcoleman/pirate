
# API Configuration
# URL of your LLM API backend
API_URL=
# LLM model to use (e.g., llama3.1:8b-instruct-q4_K_M, mistral:7b-instruct-v0.2-q4_K_M)
LLM_MODEL=

# Audio Configuration
# Speech rate: 150 for Pi (slower), 200 for Mac (faster)
SPEECH_RATE=
# Audio player: "afplay" (macOS), "aplay" (Pi), or "mpg123" (alternative)
AUDIO_PLAYER=

# Performance Settings
# Timeout for API requests (60s for Pi, 90s for Mac)
TIMEOUT=
# Wait interval for thinking phrases (5s for Pi, 3s for Mac)
WAIT_INTERVAL=

# STT Configuration
# Path to Vosk model directory
VOSK_MODEL_PATH=models/vosk-model-small-en-us-0.15
# Audio sample rate (16000 is standard)
SAMPLE_RATE=16000
# Microphone device name
# For Mac: "RØDE VideoMic NTG" or device name from list_audio_devices.py
# For Pi: "default" or specific ALSA device
MIC_DEVICE=
# Audio block size for STT (4000 for Pi, 8000 for Mac)
BLOCKSIZE=

# Prompt Configuration
# Path to the prompt file
PROMPT_FILE=prompt.txt

# =============================================================================
# EXAMPLE VALUES BY PLATFORM
# =============================================================================

# macOS Example Values:
# PLATFORM=darwin
# API_URL=http://localhost:8080/api/chat
# LLM_MODEL=llama3.1:8b-instruct-q4_K_M
# SPEECH_RATE=200
# AUDIO_PLAYER=afplay
# TIMEOUT=90
# WAIT_INTERVAL=3
# MIC_DEVICE=RØDE VideoMic NTG
# BLOCKSIZE=8000

# Raspberry Pi Example Values:
# PLATFORM=linux
# API_URL=http://localhost:8080/api/chat
# LLM_MODEL=llama3.1:8b-instruct-q4_K_M
# SPEECH_RATE=150
# AUDIO_PLAYER=aplay
# TIMEOUT=60
# WAIT_INTERVAL=5
# MIC_DEVICE=default
# BLOCKSIZE=4000 